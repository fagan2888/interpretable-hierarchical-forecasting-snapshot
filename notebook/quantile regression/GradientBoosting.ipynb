{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Regression with Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Wrapper for scikit-learn GradientBoostingRegressor which provides functionality to jointly predict several quantiles.\n",
    "    \n",
    "    An independent model is used for each quantile. However, with cascade=True, the models can be partially linked\n",
    "    by using the already predicted lower quantile for predicting the next higher quantile. Parameters for all models\n",
    "    as well as for specific models can be set using base_params and quantile_params.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    quantiles : iterable\n",
    "        Quantiles to predict, should be in ascending order.\n",
    "    base_params : dict, optional\n",
    "        Dictionary with parameter mappings to apply to all models.\n",
    "    quantile_params: dict of dicts, optional\n",
    "        Dictionary with quantiles as keys and dictionaries with parameter mappings\n",
    "        for corresponding specific models as values.\n",
    "    cascade: bool, optional\n",
    "        If True, the predicted lower quantile is used by the model for next higher quantile as well. Default is False.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self,quantiles,base_params=None,quantile_params=None,cascade=False):     \n",
    "        if base_params is None:\n",
    "            base_params = dict()\n",
    "        if quantile_params is None:\n",
    "            quantile_params = dict()\n",
    "        self.models_ = dict(zip(quantiles,[GradientBoostingRegressor(**base_params,loss=\"quantile\",alpha=q) for q in quantiles]))\n",
    "        for q,params in quantile_params.items():\n",
    "            self.models_[q] = self.models_[q].set_params(**params)\n",
    "        \n",
    "        self.quantiles = quantiles\n",
    "        self.base_params = base_params\n",
    "        self.quantile_params = quantile_params\n",
    "        self.cascade = cascade\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if not self.cascade:\n",
    "            X = np.array(X)\n",
    "            for q,m in self.models_.items():\n",
    "                m.fit(X,y)\n",
    "        else:\n",
    "            X_ = np.array(X)\n",
    "            for q,m in self.models_.items():\n",
    "                m.fit(X_,y)\n",
    "                y_ = m.predict(X_)\n",
    "                X_ = np.hstack((X_,y_.reshape(-1,1)))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.cascade:\n",
    "            X = np.array(X)\n",
    "            predictions = np.array([m.predict(X) for m in self.models_.values()]).transpose()\n",
    "        else:\n",
    "            predictions = list()\n",
    "            X_ = np.array(X)\n",
    "            for m in self.models_.values():\n",
    "                y_ = m.predict(X_)\n",
    "                predictions.append(y_)\n",
    "                X_ = np.hstack((X_,y_.reshape(-1,1)))\n",
    "            predictions = np.array(predictions).transpose()\n",
    "        return predictions\n",
    "    \n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(quantiles=self.quantiles,\n",
    "                    base_params=self.base_params,\n",
    "                    quantile_params=self.quantile_params,\n",
    "                    cascade=self.cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Wrapper for LightGBMRegressor which provides functionality to jointly predict several quantiles.\n",
    "    \n",
    "    An independent model is used for each quantile. However, with cascade=True, the models can be partially linked\n",
    "    by using the already predicted lower quantile for predicting the next higher quantile. Parameters for all models\n",
    "    as well as for specific models can be set using base_params and quantile_params.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    quantiles : iterable\n",
    "        Quantiles to predict, should be in ascending order.\n",
    "    base_params : dict, optional\n",
    "        Dictionary with parameter mappings to apply to all models.\n",
    "    quantile_params: dict of dicts, optional\n",
    "        Dictionary with quantiles as keys and dictionaries with parameter mappings\n",
    "        for corresponding specific models as values.\n",
    "    cascade: bool, optional\n",
    "        If True, the predicted lower quantile is used by the model for next higher quantile as well. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,quantiles,base_params=dict(),quantile_params=dict(),cascade=False):     \n",
    "        if base_params is None:\n",
    "            base_params = dict()\n",
    "        if quantile_params is None:\n",
    "            quantile_params = dict()\n",
    "        self.models_ = dict(zip(quantiles,[LGBMRegressor(**base_params,objective=\"quantile\",alpha=q) for q in quantiles]))\n",
    "        for q,params in quantile_params.items():\n",
    "            self.models_[q] = self.models_[q].set_params(**params)\n",
    "        \n",
    "        self.quantiles = quantiles\n",
    "        self.base_params = base_params\n",
    "        self.quantile_params = quantile_params\n",
    "        self.cascade = cascade\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if not self.cascade:\n",
    "            X = np.array(X)\n",
    "            for q,m in self.models_.items():\n",
    "                m.fit(X,y)\n",
    "        else:\n",
    "            X_ = np.array(X)\n",
    "            for q,m in self.models_.items():\n",
    "                m.fit(X_,y)\n",
    "                y_ = m.predict(X_)\n",
    "                X_ = np.hstack((X_,y_.reshape(-1,1)))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.cascade:\n",
    "            X = np.array(X)\n",
    "            predictions = np.array([m.predict(X) for m in self.models_.values()]).transpose()\n",
    "        else:\n",
    "            predictions = list()\n",
    "            X_ = np.array(X)\n",
    "            for m in self.models_.values():\n",
    "                y_ = m.predict(X_)\n",
    "                predictions.append(y_)\n",
    "                X_ = np.hstack((X_,y_.reshape(-1,1)))\n",
    "            predictions = np.array(predictions).transpose()\n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(quantiles=self.quantiles,\n",
    "                    base_params=self.base_params,\n",
    "                    quantile_params=self.quantile_params,\n",
    "                    cascade=self.cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Wrapper for CatBoostRegressor which provides functionality to jointly predict several quantiles.\n",
    "    \n",
    "    An independent model is used for each quantile. However, with cascade=True, the models can be partially linked\n",
    "    by using the already predicted lower quantile for predicting the next higher quantile. Parameters for all models\n",
    "    as well as for specific models can be set using base_params and quantile_params.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    quantiles : iterable\n",
    "        Quantiles to predict, should be in ascending order.\n",
    "    base_params : dict, optional\n",
    "        Dictionary with parameter mappings to apply to all models.\n",
    "    quantile_params: dict of dicts, optional\n",
    "        Dictionary with quantiles as keys and dictionaries with parameter mappings\n",
    "        for corresponding specific models as values.\n",
    "    cascade: bool, optional\n",
    "        If True, the predicted lower quantile is used by the model for next higher quantile as well. Default is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,quantiles,base_params=dict(),quantile_params=dict(),cascade=False):     \n",
    "        if base_params is None:\n",
    "            base_params = dict()\n",
    "        if quantile_params is None:\n",
    "            quantile_params = dict()\n",
    "        self.models_ = dict(zip(quantiles,[CatBoostRegressor(**base_params,loss_function=f\"Quantile:alpha={q}\") for q in quantiles]))\n",
    "        for q,params in quantile_params.items():\n",
    "            self.models_[q] = self.models_[q].set_params(**params)\n",
    "        \n",
    "        self.quantiles = quantiles\n",
    "        self.base_params = base_params\n",
    "        self.quantile_params = quantile_params\n",
    "        self.cascade = cascade\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if not self.cascade:\n",
    "            X = np.array(X)\n",
    "            for q,m in self.models_.items():\n",
    "                m.fit(X,y)\n",
    "        else:\n",
    "            X_ = np.array(X)\n",
    "            for q,m in self.models_.items():\n",
    "                m.fit(X_,y)\n",
    "                y_ = m.predict(X_)\n",
    "                X_ = np.hstack((X_,y_.reshape(-1,1)))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.cascade:\n",
    "            X = np.array(X)\n",
    "            predictions = np.array([m.predict(X) for m in self.models_.values()]).transpose()\n",
    "        else:\n",
    "            predictions = list()\n",
    "            X_ = np.array(X)\n",
    "            for m in self.models_.values():\n",
    "                y_ = m.predict(X_)\n",
    "                predictions.append(y_)\n",
    "                X_ = np.hstack((X_,y_.reshape(-1,1)))\n",
    "            predictions = np.array(predictions).transpose()\n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return dict(quantiles=self.quantiles,\n",
    "                    base_params=self.base_params,\n",
    "                    quantile_params=self.quantile_params,\n",
    "                    cascade=self.cascade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    noro_ts = pd.read_pickle(\"../../data/processed/norovirus-ts.pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_window = 5\n",
    "    forecast_horizon = 1\n",
    "    ts_shifted = pd.concat([noro_ts[[\"Count\"]].shift(i).rename(columns={\"Count\":f\"Count_t-{i}\"}) for i in range(input_window)],axis=1)\n",
    "    ts_shifted[\"target\"] = ts_shifted[\"Count_t-0\"].shift(-forecast_horizon)\n",
    "    ts_shifted = ts_shifted.dropna()\n",
    "    X = ts_shifted.drop(\"target\",axis=1)\n",
    "    y = ts_shifted[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242.87767706519676\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    gbqr = GradientBoostingQuantileRegressor(quantiles=[0.05,0.5,0.95],\n",
    "                                             base_params=dict(n_estimators=102),\n",
    "                                             quantile_params={0.5:dict(n_estimators=200)},\n",
    "                                             cascade=False)\n",
    "    gbqr = gbqr.fit(X,y)\n",
    "    y_hat = gbqr.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273.90630672688025\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    gbqr = GradientBoostingQuantileRegressor(quantiles=[0.05,0.5,0.95])\n",
    "    gbqr = gbqr.fit(X,y)\n",
    "    y_hat = gbqr.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238.06621752034613\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gbqr_casc = GradientBoostingQuantileRegressor(quantiles=[0.05,0.5,0.95],\n",
    "                                             base_params=dict(n_estimators=102),\n",
    "                                             quantile_params={0.5:dict(n_estimators=200)},\n",
    "                                             cascade=True)\n",
    "    gbqr_casc = gbqr_casc.fit(X,y)\n",
    "    y_hat = gbqr_casc.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.766501362973\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lgbmqr = LightGBMQuantileRegressor(quantiles=[0.05,0.5,0.95],\n",
    "                               base_params=dict(n_estimators=101,seed=np.random.randint(100)),\n",
    "                               quantile_params={0.5:dict(n_estimators=200)},\n",
    "                               cascade=False)\n",
    "    lgbmqr = lgbmqr.fit(X,y)\n",
    "    y_hat = lgbmqr.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198.06629715684875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lgbmqr_casc = LightGBMQuantileRegressor(quantiles=[0.05,0.5,0.95],\n",
    "                               base_params=dict(n_estimators=101,seed=np.random.randint(100)),\n",
    "                               quantile_params={0.5:dict(n_estimators=200)},\n",
    "                               cascade=True)\n",
    "    lgbmqr_casc = lgbmqr_casc.fit(X,y)\n",
    "    y_hat = lgbmqr_casc.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326.15965320374676\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cbqr = CatBoostQuantileRegressor(quantiles=[0.05,0.5,0.95],\n",
    "                               base_params=dict(n_estimators=101,silent=True, \n",
    "                                    random_seed=np.random.randint(100)),\n",
    "                               quantile_params={0.5:dict(n_estimators=200)},\n",
    "                               cascade=False)\n",
    "    cbqr = cbqr.fit(X,y)\n",
    "    y_hat = cbqr.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329.89432820282417\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cbqr_casc = CatBoostQuantileRegressor(quantiles=[0.05,0.5,0.95],\n",
    "                               base_params=dict(n_estimators=101,silent=True,\n",
    "                                        random_seed=np.random.randint(100)),\n",
    "                               quantile_params={0.5:dict(n_estimators=200)},\n",
    "                               cascade=True)\n",
    "    cbqr_casc = cbqr_casc.fit(X,y)\n",
    "    y_hat = cbqr_casc.predict(X)\n",
    "    # Compute in-sample MAE for median\n",
    "    print(np.mean(np.abs(y_hat[:,1]-y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda4841bd74195e4398914578314ee45bf3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
