{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for evaluation of scan logs from literature research\n",
    "This script processes literature research logs by creating a\n",
    "CSV file and a readme file for overview over the literature\n",
    "scan. Moreover, it extracts the bibtex info of all positively\n",
    "scanned publications into a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_directory = \"Hierarchical Forecasting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import Levenshtein as lev\n",
    "import itertools\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "from bibtexparser.customization import convert_to_unicode, homogenize_latex_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_to_df(scan, finished=True):\n",
    "    BODY_SEPARATOR = \"#-------------------------\\nDETAILS ------------------\\n#-------------------------\"\n",
    "    BIBTEX_PATTERN = \"(.+?) ?= ?\\{(.*?)\\}\"\n",
    "    PUBLICATION_PATTERN = \"@[A-Za-z]+?\\{.*?\\}\\n\\n\"\n",
    "\n",
    "    scan_header = scan.split(sep=BODY_SEPARATOR)[0]\n",
    "    scan_body = scan.split(sep=BODY_SEPARATOR)[1]\n",
    "\n",
    "    header_infos = {k: v for k, v in re.findall(BIBTEX_PATTERN, scan_header)}\n",
    "    header_infos[\"found\"] = int(header_infos[\"found\"])\n",
    "    try:\n",
    "        header_infos[\"kept_after_scan\"] = int(header_infos[\"kept_after_scan\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    publications_raw = re.findall(PUBLICATION_PATTERN, scan_body, re.DOTALL)\n",
    "    publications = [\n",
    "        {k: v for k, v in re.findall(BIBTEX_PATTERN, publication)}\n",
    "        for publication in publications_raw\n",
    "    ]\n",
    "\n",
    "    # check integrity\n",
    "    if finished:\n",
    "        # is number of found publications equal to reported number of publications?\n",
    "        if len(publications) != header_infos[\"found\"]:\n",
    "            raise UserWarning(\n",
    "                \"The number of found publications does not match the reported number of publications.\"\n",
    "            )\n",
    "\n",
    "        # have all publications been tagged correctly with a \"take\" decision?\n",
    "        for publication in publications:\n",
    "            if \"take\" not in publication:\n",
    "                raise UserWarning(\n",
    "                    f\"Publication {publication['title']} has not valid take tag\"\n",
    "                )\n",
    "\n",
    "    # make dataframe\n",
    "    publicationFrame = pd.DataFrame(publications)\n",
    "\n",
    "    publicationFrame.columns = [col.lower() for col in publicationFrame.columns]\n",
    "\n",
    "    if finished:\n",
    "        take = pd.DataFrame(\n",
    "            publicationFrame[\"take\"]\n",
    "            .apply(lambda x: re.findall(\"(.+?)\\.(.*)\", x)[0])\n",
    "            .tolist()\n",
    "        )\n",
    "        publicationFrame[\"take\"] = take[0]\n",
    "        publicationFrame[\"take_explanation\"] = take[1].str.strip()\n",
    "\n",
    "        if set(publicationFrame[\"take\"]) != set([\"Yes\", \"No\"]):\n",
    "            raise AssertionError(\"Not all take tags could be classified as Yes or No.\")\n",
    "\n",
    "        publicationFrame[\"take\"] = publicationFrame[\"take\"] == \"Yes\"\n",
    "\n",
    "        for col in publicationFrame.columns:\n",
    "            publicationFrame.loc[publicationFrame[col].isnull(), col] = \"\"\n",
    "\n",
    "    return publicationFrame, header_infos, publications_raw\n",
    "\n",
    "\n",
    "def pub_to_md(pub):\n",
    "    source_tag = \"\"\n",
    "    if \"journal\" in pub: source_tag=f\"*{pub['journal'].strip()}*\"\n",
    "    if source_tag in [\"\",\"**\"] and \"booktitle\" in pub: source_tag=f\"*{pub['booktitle'].strip()}*\"\n",
    "        \n",
    "    res = f\"### **{pub['title'].strip()}**\\n{pub['author'].strip()}. ({pub['year']}). {pub['title'].strip()}. {source_tag}\"\n",
    "    if pub[\"take_explanation\"] != \"\":\n",
    "        res += \"\\n\\n**Comment:** \" + (pub[\"take_explanation\"].strip())\n",
    "    return res\n",
    "\n",
    "\n",
    "def df_to_AccDis(pubframe):\n",
    "    accepted = \"\\n\\n\".join(\n",
    "        [pub_to_md(pub) for k, pub in pubframe[pubframe[\"take\"].str.lower().str.contains(\"yes\")].iterrows()]\n",
    "    )\n",
    "    discarded = \"\\n\\n\".join(\n",
    "        [pub_to_md(pub) for k, pub in pubframe[pubframe[\"take\"].str.lower().str.contains(\"no\")].iterrows()]\n",
    "    )\n",
    "    to_decide = \"\\n\\n\".join(\n",
    "        [pub_to_md(pub) for k, pub in pubframe[pubframe[\"take\"]==\"\"].iterrows()]\n",
    "    )\n",
    "    return accepted, discarded, to_decide\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, \"r\", encoding=\"latin-1\") as f:\n",
    "        file = f.read()\n",
    "    return file\n",
    "\n",
    "\n",
    "def write_file(text, path):\n",
    "    with open(path, \"w\", encoding=\"latin-1\") as f:\n",
    "        f.write(text)\n",
    "        \n",
    "def already_tagged(new, already, threshold=4):\n",
    "    new1 = new[[\"title\"]].assign(key=1).reset_index()\n",
    "    already1 = (\n",
    "        already[[\"title\", \"take\", \"take_explanation\"]].assign(key=1).reset_index()\n",
    "    )\n",
    "    compare = pd.merge(new1, already1, on=\"key\", suffixes=(\"_new\", \"_already\"))\n",
    "    compare[\"titles\"] = tuple(zip(compare[\"title_new\"], compare[\"title_already\"]))\n",
    "    compare[\"dist\"] = compare[\"titles\"].apply(lambda x: lev.distance(x[0], x[1]))\n",
    "    return compare[compare[\"dist\"] <= threshold].reset_index()[\n",
    "        [\n",
    "            \"index_new\",\n",
    "            \"title_new\",\n",
    "            \"title_already\",\n",
    "            \"index_already\",\n",
    "            \"dist\",\n",
    "            \"take\",\n",
    "            \"take_explanation\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "def update_scan(new_scan, new_raw, already, already_header_info):\n",
    "    for _, v in already.iterrows():\n",
    "        insert = (\n",
    "            (\"Yes.\" if v[\"take\"] else \"No.\") + \" \" + v[\"take_explanation\"]\n",
    "        ).strip()\n",
    "        first_seen = already_header_info[\"date\"] + \" \" + already_header_info[\"where\"]\n",
    "        new_scan = new_scan.replace(\n",
    "            new_raw[v[\"index_new\"]],\n",
    "            new_raw[v[\"index_new\"]].replace(\n",
    "                \"\\n}\\n\\n\", f\"\\ntake={{{insert}}},\\nfirst_seen={{{first_seen}}},\\n}}\\n\\n\"\n",
    "            ),\n",
    "        )\n",
    "    return new_scan\n",
    "\n",
    "def export_readme(path,df,header_infos):\n",
    "    accepted, discarded, to_decide = df_to_AccDis(df.fillna(\"\").astype(\"str\"))\n",
    "    pending = '\\n\\n-----\\n\\n## **Pending Publications**\\n\\n'+to_decide if to_decide!='' else ''\n",
    "    # write README file\n",
    "    readme_text = f\"# **Literature Research Scan**\\n\\n**Source:** {header_infos['where']}\\n\\n**Date:** {header_infos['date']}\\n\\n**Search Terms:** {header_infos['terms']}\\n\\n**Search Criteria:** {header_infos['criterion']}\\n\\n**Results:** {header_infos['found']} publications were scanned by title and abstract, {header_infos['kept_after_scan']} were kept. {header_infos['notes']}\\n\\n-----\\n\\n## **Accepted Publications**\\n\\n{accepted}\\n\\n-----\\n\\n## **Discarded Publications**\\n\\n{discarded}{pending}\"\n",
    "    write_file(readme_text, path)\n",
    "    \n",
    "def export_summary_readme(path,df,header_infos):\n",
    "    accepted, discarded, to_decide = df_to_AccDis(df.fillna(\"\").astype(\"str\"))\n",
    "    pending = '\\n\\n-----\\n\\n## **Pending Publications**\\n\\n'+to_decide if to_decide!='' else ''\n",
    "    # write README file\n",
    "    readme_text = f\"# **Summary of Literature Research Scan**\\n\\n**Sources** (links to individual scan summaries): {header_infos['where']}\\n\\n**Search Terms:** {header_infos['terms']}\\n\\n**Search Criteria:** {header_infos['criterion']}\\n\\n**Results:** Overall, {header_infos['found']} different publications were scanned by title and abstract, {header_infos['kept_after_scan']} were kept.\\n\\n-----\\n\\n## **Accepted Publications**\\n\\n{accepted}\\n\\n-----\\n\\n## **Discarded Publications**\\n\\n{discarded}{pending}\"\n",
    "    write_file(readme_text, path)\n",
    "    \n",
    "def get_bib_duplicates(df):\n",
    "    dups =  df.loc[[subitem for item in [(x[0][1],x[1][1]) for x in list(itertools.combinations(df[[\"title\",\"index\"]].values.tolist(),2)) if lev.distance(x[0][0].lower(), x[1][0].lower())<4] for subitem in item]]\n",
    "    return dups\n",
    "    \n",
    "def load_bibtex(path, verbatim=True):\n",
    "    if verbatim: print(path)\n",
    "    with open(path, \"r\", encoding=\"latin-1\") as bibtex_file:\n",
    "        bib_database = bibtexparser.load(bibtex_file)\n",
    "        \n",
    "    header_infos = bib_database.strings\n",
    "    \n",
    "    df = pd.DataFrame(bib_database.entries)\n",
    "    df = df.assign(path=path)\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    if \"take\" not in df.columns:\n",
    "        df[\"take\"]=None\n",
    "        df[\"take_explanation\"]=None\n",
    "        \n",
    "    df[\"title\"] = df[\"title\"].str.replace(\"{\",\"\").str.replace(\"}\",\"\").str.replace(\"\\n\",\" \")\n",
    "    \n",
    "    return bib_database, header_infos, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very important columns (for inspection)\n",
    "vic = [\"index\",\"title\",\"author\",\"journal\",\"take\",\"take_explanation\",\"path\",\"match_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('Hierarchical Forecasting/ACM Digital Library/scan.bib'),\n",
       " WindowsPath('Hierarchical Forecasting/EBSCO Host/scan.bib'),\n",
       " WindowsPath('Hierarchical Forecasting/Scopus/scan.bib'),\n",
       " WindowsPath('Hierarchical Forecasting/Web of Science/scan.bib')]"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = list(Path(start_directory).rglob(\"*.bib\"))\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Forecasting\\ACM Digital Library\\scan.bib\n",
      "Hierarchical Forecasting\\EBSCO Host\\scan.bib\n",
      "Hierarchical Forecasting\\Scopus\\scan.bib\n",
      "Hierarchical Forecasting\\Web of Science\\scan.bib\n"
     ]
    }
   ],
   "source": [
    "#sources = [\"20_04_16 Nowcasting/Scopus/scan2.bib\",\"20_04_16 Nowcasting/EBSCO/scan2.bib\"]\n",
    "bib_list,header_list,df_list = (zip(*[load_bibtex(source) for source in sources])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hierarchical Forecasting\\ACM Digital Library\\scan.bib    []\n",
       "Hierarchical Forecasting\\EBSCO Host\\scan.bib             []\n",
       "Hierarchical Forecasting\\Scopus\\scan.bib                 []\n",
       "Hierarchical Forecasting\\Web of Science\\scan.bib         []\n",
       "dtype: object"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any entries which have been processed as comments, which may indicate a problem\n",
    "pd.Series({str(source):bib.comments for bib,source in zip(bib_list,sources)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hierarchical Forecasting\\ACM Digital Library\\scan.bib    0\n",
       "Hierarchical Forecasting\\EBSCO Host\\scan.bib             0\n",
       "Hierarchical Forecasting\\Scopus\\scan.bib                 0\n",
       "Hierarchical Forecasting\\Web of Science\\scan.bib         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show number of duplicates in bibs\n",
    "pd.Series({str(source):get_bib_duplicates(df).shape[0] for df,source in zip(df_list,sources)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title]\n",
       "Index: []"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bib_duplicates(df_list[0])[[\"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 matching pairs.\n"
     ]
    }
   ],
   "source": [
    "max_dist = 7\n",
    "all_entries = pd.concat(df_list,sort=False,ignore_index=True)\n",
    "all_indexes = [all_entries.index[all_entries[\"path\"]==path].tolist() for path in all_entries[\"path\"].unique()]\n",
    "to_compare = [item for list_comb in list(itertools.combinations(all_indexes,2)) for item in list(itertools.product(*list_comb))]\n",
    "dists = {x:lev.distance(all_entries.loc[x[0],\"title\"].lower(), all_entries.loc[x[1],\"title\"].lower()) for x in to_compare}\n",
    "small_dists = {i:(all_entries.loc[x[0],\"title\"],all_entries.loc[x[1],\"title\"],d,x[0],x[1],all_entries.loc[x[0],\"path\"],all_entries.loc[x[1],\"path\"]) for i, (x,d) in enumerate(dists.items()) if d<max_dist}\n",
    "print(f\"Found {len(small_dists)} matching pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show most distant matches for quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_x</th>\n",
       "      <th>title_y</th>\n",
       "      <th>distance</th>\n",
       "      <th>index_x</th>\n",
       "      <th>index_y</th>\n",
       "      <th>path_x</th>\n",
       "      <th>path_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2313</td>\n",
       "      <td>Grouped multivariate and functional time series forecasting:An application to annuity pricing.</td>\n",
       "      <td>Grouped multivariate and functional time series forecasting: An application to annuity pricing</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>90</td>\n",
       "      <td>Hierarchical Forecasting\\EBSCO Host\\scan.bib</td>\n",
       "      <td>Hierarchical Forecasting\\Web of Science\\scan.bib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>Grouped multivariate and functional time series forecasting:An application to annuity pricing.</td>\n",
       "      <td>Grouped multivariate and functional time series forecasting: An application to annuity pricing</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>61</td>\n",
       "      <td>Hierarchical Forecasting\\EBSCO Host\\scan.bib</td>\n",
       "      <td>Hierarchical Forecasting\\Scopus\\scan.bib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2434</td>\n",
       "      <td>Forecasting of a Hierarchical Functional Time Series on Example of Macromodel for the Day and Night Air Pollution in Silesia Region -- A Critical Overview.</td>\n",
       "      <td>Forecasting of a Hierarchical Functional Time Series on Example of Macromodel for the Day and Night Air Pollution in Silesia Region - A Critical Overview</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>95</td>\n",
       "      <td>Hierarchical Forecasting\\EBSCO Host\\scan.bib</td>\n",
       "      <td>Hierarchical Forecasting\\Web of Science\\scan.bib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1346</td>\n",
       "      <td>Forecasting of a Hierarchical Functional Time Series on Example of Macromodel for the Day and Night Air Pollution in Silesia Region -- A Critical Overview.</td>\n",
       "      <td>Forecasting of a hierarchical functional time series on example of macromodel for the day and night air pollution in silesia region - A critical overview</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>50</td>\n",
       "      <td>Hierarchical Forecasting\\EBSCO Host\\scan.bib</td>\n",
       "      <td>Hierarchical Forecasting\\Scopus\\scan.bib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                          title_x  \\\n",
       "2313  Grouped multivariate and functional time series forecasting:An application to annuity pricing.                                                                \n",
       "1173  Grouped multivariate and functional time series forecasting:An application to annuity pricing.                                                                \n",
       "2434  Forecasting of a Hierarchical Functional Time Series on Example of Macromodel for the Day and Night Air Pollution in Silesia Region -- A Critical Overview.   \n",
       "1346  Forecasting of a Hierarchical Functional Time Series on Example of Macromodel for the Day and Night Air Pollution in Silesia Region -- A Critical Overview.   \n",
       "\n",
       "                                                                                                                                                        title_y  \\\n",
       "2313  Grouped multivariate and functional time series forecasting: An application to annuity pricing                                                              \n",
       "1173  Grouped multivariate and functional time series forecasting: An application to annuity pricing                                                              \n",
       "2434  Forecasting of a Hierarchical Functional Time Series on Example of Macromodel for the Day and Night Air Pollution in Silesia Region - A Critical Overview   \n",
       "1346  Forecasting of a hierarchical functional time series on example of macromodel for the day and night air pollution in silesia region - A critical overview   \n",
       "\n",
       "      distance  index_x  index_y  \\\n",
       "2313  2         14       90        \n",
       "1173  2         14       61        \n",
       "2434  2         18       95        \n",
       "1346  2         18       50        \n",
       "\n",
       "                                            path_x  \\\n",
       "2313  Hierarchical Forecasting\\EBSCO Host\\scan.bib   \n",
       "1173  Hierarchical Forecasting\\EBSCO Host\\scan.bib   \n",
       "2434  Hierarchical Forecasting\\EBSCO Host\\scan.bib   \n",
       "1346  Hierarchical Forecasting\\EBSCO Host\\scan.bib   \n",
       "\n",
       "                                                path_y  \n",
       "2313  Hierarchical Forecasting\\Web of Science\\scan.bib  \n",
       "1173  Hierarchical Forecasting\\Scopus\\scan.bib          \n",
       "2434  Hierarchical Forecasting\\Web of Science\\scan.bib  \n",
       "1346  Hierarchical Forecasting\\Scopus\\scan.bib          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matches = pd.DataFrame.from_dict(small_dists,orient=\"index\",columns=[\"title_x\",\"title_y\",\"distance\",\"index_x\",\"index_y\",\"path_x\",\"path_y\"])\n",
    "with pd.option_context('display.max_colwidth', -1):\n",
    "    display(matches.sort_values(\"distance\",ascending=False).query(\"distance>1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add match-IDs and check for unilateral matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 30 different titles\n"
     ]
    }
   ],
   "source": [
    "all_entries[\"match_id\"] = [[] for _ in range(len(all_entries))]\n",
    "for x,d in dists.items():\n",
    "    if d<4:\n",
    "        all_entries.loc[x[0],\"match_id\"].extend(x)\n",
    "        all_entries.loc[x[1],\"match_id\"].extend(x)\n",
    "all_entries[\"match_id\"]=[str(sorted(list(set(entry)))) for entry in all_entries[\"match_id\"]]\n",
    "all_entries[\"match_count\"] = all_entries.groupby(\"match_id\")[\"take\"].transform(lambda x: len(x))\n",
    "print(f\"Matched {len(all_entries['match_id'].unique())-1} different titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [index, year, url, title, take_explanation, take, series, publisher, path, pages, numpages, location, keywords, isbn, doi, booktitle, author, address, ENTRYTYPE, ID, articleno, volume, number, month, journal, issue_date, issn, abstract, source, note, document_type, author_keywords, affiliation, art_number, unique-id, researcherid-numbers, orcid-numbers, eissn, article-number, match_id, match_count]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "------------------------------------------------------\n",
      "Series([], Name: take, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Show entries which have not been correctly matched\n",
    "print(all_entries.query(\"match_count<2\"))\n",
    "print(\"------------------------------------------------------\")\n",
    "print(all_entries.groupby(\"match_id\")[\"take\"].size().pipe(lambda x: x[x<2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy take and take_explanation for all matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_func(x):\n",
    "    if len(x.dropna().unique())==1: return x.dropna().iloc[0]\n",
    "    if len(x.dropna().unique())>1: raise AssertionError(x)\n",
    "    else: return None\n",
    "\n",
    "all_entries.loc[all_entries[\"match_id\"]!=\"[]\",\"take\"] = all_entries.loc[all_entries[\"match_id\"]!=\"[]\"].groupby(\"match_id\")[\"take\"].transform(copy_func)\n",
    "all_entries.loc[all_entries[\"match_id\"]!=\"[]\",\"take_explanation\"] = all_entries.loc[all_entries[\"match_id\"]!=\"[]\"].groupby(\"match_id\")[\"take_explanation\"].transform(copy_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update bibs and write to file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Hierarchical Forecasting\\ACM Digital Library\\scan.bib\n",
      "Exporting Hierarchical Forecasting\\EBSCO Host\\scan.bib\n",
      "Exporting Hierarchical Forecasting\\Scopus\\scan.bib\n",
      "Exporting Hierarchical Forecasting\\Web of Science\\scan.bib\n"
     ]
    }
   ],
   "source": [
    "for bib,df,header,source in zip(bib_list,df_list,header_list,sources):\n",
    "    df_updated = df.drop([\"take\",\"take_explanation\"],axis=1,errors='ignore').merge(all_entries[[\"index\",\"take\",\"take_explanation\",\"path\"]],on=[\"index\",\"path\"],how=\"left\")\n",
    "    df_updated = df_updated.drop([\"index\"],axis=1)\n",
    "    \n",
    "    header['found'] = str(df_updated.shape[0])\n",
    "    header['kept_after_scan'] = str(df_updated[\"take\"].str.lower().str.contains(\"yes\").sum())\n",
    "    header['discarded_after_scan'] = str((1-df_updated[\"take\"].str.lower().str.contains(\"yes\")).sum())\n",
    "    header['not_yet_decided'] = str(df_updated[\"take\"].isnull().sum())\n",
    "    \n",
    "    df_updated[\"to_decide\"]=None\n",
    "    df_updated.loc[df_updated[\"take\"].isnull(),\"to_decide\"]=\"Yes\"\n",
    "    \n",
    "    bib.strings = header\n",
    "    \n",
    "    bib.entries = [{k:str(v) for k,v in m.items() if pd.notnull(v)} for m in df_updated.to_dict('records')]\n",
    "    with open(source, 'w', encoding=\"latin-1\") as bibtex_file:\n",
    "        bibtexparser.dump(bib, bibtex_file)\n",
    "    \n",
    "    print(f\"Exporting {source}\")\n",
    "    df.to_csv(str(source).replace(\".bib\",\".csv\"), index=False)\n",
    "    export_readme(str(source).replace(os.path.basename(source),\"readme.md\"),df_updated,header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export summary from all databases which have been searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.concat([all_entries[all_entries[\"match_id\"]!=\"[]\"].drop_duplicates(subset=\"match_id\"),all_entries[all_entries[\"match_id\"]==\"[]\"]]).drop([\"index\"],axis=1)\n",
    "def to_rel_path(s,base):\n",
    "    return str(s).replace(\"\\\\\",\"/\").replace(base,\"\").replace(\" \",\"%20\").replace(os.path.basename(s),\"readme.md\")\n",
    "                     \n",
    "header = {\"where\":\"\\n-  \"+\"\\n - \".join([f\"[{h['where']}](.{to_rel_path(s,start_directory)})\" for h,s in zip(header_list,sources)]),\n",
    "         \"terms\":header_list[-1][\"terms\"],\"criterion\":header_list[-1][\"criterion\"]}\n",
    "header['found'] = str(summary.shape[0])\n",
    "header['kept_after_scan'] = str(summary[\"take\"].str.lower().str.contains(\"yes\").sum())\n",
    "header['discarded_after_scan'] = str((1-summary[\"take\"].str.lower().str.contains(\"yes\")).sum())\n",
    "export_summary_readme(f\"{start_directory}/readme.md\",summary,header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda4841bd74195e4398914578314ee45bf3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
